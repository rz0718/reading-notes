{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra and Learning from Data\n",
    "\n",
    "This is one recent book from Prof. Gilbert Strang.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter One Highlights of linear algebra\n",
    "\n",
    "1. $\\mathbf{A}x$ is a linear combination of the columns of A. This is fundamental. It is also the column space of A\n",
    "\n",
    "2. Three independent columns in $\\mathbf{R}^3$ produce an invertible matrix A. And $\\mathbf{A}x=0$ require x=0 and $\\mathbf{A}x=b$ has exactly one solution.\n",
    "\n",
    "3. Factorization: $\\mathbf{A}=\\mathbf{R}\\mathbf{C}$. (m by n)=(m by r) by (r by n). Here, $\\mathbf{C}$ will be independent columns in A. SVD of A is that C contain r orthogonal columns and R has r orthogonal rows.\n",
    "\n",
    "4. $\\mathbf{A}\\mathbf{B}$ can be regarded as the sum of columns-row multiplication i.e. rank-one matrices. It is essential in data science. The short answer is: we are looking for the important part of a matrix $\\mathbf{A}$. \n",
    "\n",
    "5. Matrix Factorization:\n",
    "\n",
    "    *  $\\mathbf{A} = \\mathbf{L}\\mathbf{U}$ (from elimination)\n",
    "    *  $\\mathbf{A} = \\mathbf{Q}\\mathbf{R}$ (from orthogonalizing, i.e.m Gram-Schmidt). Here, Q has orthonormal columns.\n",
    "    * $\\mathbf{S} = \\mathbf{Q}diag\\mathbf{Q}^T$. Eigenvalues and orthonormal eigenvectors are in the matrices. It is also called spectral theorm.\n",
    "    * $\\mathbf{A} = \\mathbf{X}diag\\mathbf{X}^{-1}$. is **diagonalization** when A is n by n matrix with n independent eigenvectors. \n",
    "    * SVD: $\\mathbf{A} = \\mathbf{U}\\sum\\mathbf{V}^T$. Here, $\\mathbf{U}\\mathbf{U}^T=\\mathbf{I}$ and $\\mathbf{V}\\mathbf{V}^T=\\mathbf{I}$\n",
    "    \n",
    "<img src=\"img_algebra/big_pic.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "6. Ranks Formulation:\n",
    "\n",
    "    * Rank of AB <= Rank of A or Rank of B\n",
    "    * Rank of (A+B) <= Rank of A + Rank of B\n",
    "    * Rank of $\\mathbf{A}^T\\mathbf{A}$ = $\\mathbf{A}\\mathbf{A}^T$ = $\\mathbf{A}$ = $\\mathbf{A}^T$\n",
    "    * If A is m by r and B is r by n and their rank are both r, AB also has rank r.\n",
    "    \n",
    "7. $\\mathbf{A}=\\mathbf{L}\\mathbf{U}$ is the matrix description of elimination without row exchanges. We developed A=LU into a lower triangular L times an upper triangular U. Bascially, $\\mathbf{A}x=\\mathbf{b}$ split into $L\\mathbf{c}=\\mathbf{b}$ and $\\mathbf{U}x=\\mathbf{c}$.  \n",
    "\n",
    "8. If permutations, every invertible n by n matrix $\\mathbf{A}$ leads to $\\mathbf{P}\\mathbf{A}=\\mathbf{L}\\mathbf{U}$\n",
    "\n",
    "9. Every subspace of $R^n$ has an orthogonal basis. For example, given two vectors a and b, we can have the basis as:  a and $\\mathbf{c}=\\mathbf{b}-\\frac{\\mathbf{a}^T\\mathbf{b}}{\\mathbf{a}^T\\mathbf{a}}\\mathbf{a}$\n",
    "\n",
    "10. Orthogonal matrices denote rotations of the plane or reflections.\n",
    "\n",
    "11. Eigenvectors is the basis for the matrix: $\\mathbf{A}^k\\mathbf{v}=\\sum_{i=1}^nc_i\\lambda_i^k\\mathbf{x_i}$ if $\\mathbf{v}=\\sum_{i=1}^nc_i\\mathbf{x}_i$\n",
    "\n",
    "12. Four properties of eigenvalues and eigenvectors:\n",
    "    * Trace is the sum of eigenvalues\n",
    "    * Determinant is the productions of eigenvalues\n",
    "    * Symmetric matrices have real eigenvalues\n",
    "    * If the eigenvalues are not equal for symmetrix matrix, eigenvectors are orthogonal\n",
    "    * For non-symmetrix matrix, eigenvectors are orthogonal if $\\mathbf{A}^T\\mathbf{A}=\\mathbf{A}\\mathbf{A}^T$\n",
    "    \n",
    "13. **Similar Matrix**: If $\\mathbf{A}\\mathbf{x}=\\lambda\\mathbf{x}$ then $\\mathbf{BAB^{-1}}(\\mathbf{Bx})=\\lambda\\mathbf{Bx}$\n",
    "\n",
    "14. Diagonalizing matrix: $\\mathbf{A}=\\mathbf{X}DIAG\\mathbf{X^{-1}}$ and $\\mathbf{A^k}=\\mathbf{X}DIAG^k\\mathbf{X^{-1}}$\n",
    "\n",
    "15. Symmetric Matrices: eigenvalues are real numbers and eigenvectors could be chosen as orthogonal. $\\mathbf{S}=\\mathbf{Q}DIAG\\mathbf{Q^{T}}$\n",
    "\n",
    "16. Symmetric Positive Definite Matrices:\n",
    "    * All eigvenvalues are positive\n",
    "    * $\\mathbf{x^T}\\mathbf{S}\\mathbf{x}$ is positive for all vectors $\\mathbf{x} \\neq 0$\n",
    "    * $\\mathbf{S}=\\mathbf{A^T}\\mathbf{A}$ for a matrix $\\mathbf{A}$ with independent columns\n",
    "    * All the leading determinants of S are positive\n",
    "    * All the pivots of $\\mathbf{S}$ are positive (in elminations) $\\mathbf{S}=\\mathbf{L}\\mathbf{U}=\\mathbf{L}\\mathbf{D}\\mathbf{L}^T$ \n",
    "    \n",
    "17. For minimum point x of any function f(x), this is calculus meets linear algebra:\n",
    "\n",
    "    * Calculus: The partial derivatives of f are all zeros\n",
    "    * Linear Algebra: The matrix S of second derivatives is $\\frac{\\partial^2 f}{\\partial x_i\\partial x_j}$ is positive definite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt5153",
   "language": "python",
   "name": "bt5153"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
